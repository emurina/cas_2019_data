{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"06_sentiment_analysis_with_imdb_reviews.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"lj0kayjfOfdv","colab_type":"text"},"source":["## Sentiment analysis with imdb reviews\n","\n","In this notebook we work with the IMDb dataset, it is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive (1) or negative (0). The dataset contains an even number of positive and negative reviews. Only highly polarizing reviews are considered. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. We will apply a very simple preprocessing to the textreviews and then train a baseline randomforest on bag of words features. We will compare the results of the bag of word randomforest with a neural network performace where we learn a  dense word-embedding for each word and then classify it to either positive (1) or negative (0). Finally we will use an inception-like architecture with 1D convolutions and globalpooling and see if we can improve the performace. You can test the trained network on new reviews from the internet or by writting your own review for a movie you like or don't like."]},{"cell_type":"code","metadata":{"id":"T30znDyfOfdz","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import re\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rwiaITdXOfd7","colab_type":"code","colab":{}},"source":["# Downloading the review and labels, if it does not exist\n","import urllib\n","import os\n","if not os.path.isfile('movie_data.csv'):\n","    urllib.request.urlretrieve(\n","    \"https://www.dropbox.com/s/kvwi2nlrtk7axn9/movie_data.csv?dl=1\",\n","    \"movie_data.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JTuYfofUOfeB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":195},"outputId":"d03e6c8a-23ad-45a5-a8df-b680c33173f7","executionInfo":{"status":"ok","timestamp":1572359871192,"user_tz":-60,"elapsed":5432,"user":{"displayName":"Elvis Murina","photoUrl":"","userId":"06600021592491010883"}}},"source":["df = pd.read_csv('movie_data.csv', encoding='utf-8')\n","df[0:5]"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I went and saw this movie last night after bei...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Actor turned director Bill Paxton follows up h...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>As a recreational golfer with some knowledge o...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I saw this film in a sneak preview, and it is ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Bill Paxton has taken the true story of the 19...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review  sentiment\n","0  I went and saw this movie last night after bei...          1\n","1  Actor turned director Bill Paxton follows up h...          1\n","2  As a recreational golfer with some knowledge o...          1\n","3  I saw this film in a sneak preview, and it is ...          1\n","4  Bill Paxton has taken the true story of the 19...          1"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"dhE_jotWOfeL","colab_type":"text"},"source":["shuffle the data and extract the first 15000 reviews with the sentiments\n"]},{"cell_type":"code","metadata":{"id":"QKY_bQmFOfeN","colab_type":"code","colab":{}},"source":["df=df.sample(frac=1,random_state=22).reset_index(drop=True)\n","X = df.loc[0:14999, 'review'].values\n","y = df.loc[0:14999, 'sentiment'].values"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WKm_KAAzOfeU","colab_type":"text"},"source":["look at one review before we apply some text preprocessing"]},{"cell_type":"code","metadata":{"id":"8n2VJcDPOfeV","colab_type":"code","colab":{}},"source":["i=0\n","print(X[i])\n","print(y[i])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tnwg7vp6Ofec","colab_type":"text"},"source":["Here we do a very simple preprocessing, no stemming no lemmatization no stopwords removed  \n","codecredit: https://stackabuse.com/text-classification-with-python-and-scikit-learn/"]},{"cell_type":"code","metadata":{"id":"GW0zbtIlOfed","colab_type":"code","colab":{}},"source":["documents = []\n","\n","for i in range(0, len(X)):  \n","    # Remove all the special characters\n","    document = re.sub(r'\\W', ' ', str(X[i]))\n","    # remove all single characters\n","    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n","    # Remove single characters from the start\n","    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n","    # Substituting multiple spaces with single space\n","    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n","    # Removing prefixed 'b'\n","    document = re.sub(r'^b\\s+', '', document)\n","    # Removing html stuff\n","    document = re.sub(\"br\", '', document)\n","    # Substituting multiple spaces with single space\n","    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n","    # Converting to Lowercase\n","    document = document.lower()\n","    documents.append(document)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mw8UKcGTOfeh","colab_type":"text"},"source":["look at the review from before after the text preprocessing"]},{"cell_type":"code","metadata":{"id":"4gQyS5HROfej","colab_type":"code","colab":{}},"source":["documents[0]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CWFpB4WTOfeu","colab_type":"text"},"source":["## RF Baseline"]},{"cell_type":"markdown","metadata":{"id":"YEFN9cp2Ofet","colab_type":"text"},"source":["Here we \"vectorize\" the bag of words. We choose to consider only the 6000 most frequent words and take only words that appear in at least 5 diffrent reviews, furthermore we ignore all words that appear in more than 70% of all reviews. "]},{"cell_type":"code","metadata":{"id":"keboNZRTOfev","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import CountVectorizer  \n","vectorizer = CountVectorizer(max_features=6000, min_df=5, max_df=0.7)\n"," \n","X = vectorizer.fit_transform(documents).toarray()  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5KVtAwNMOfez","colab_type":"code","colab":{}},"source":["print(len(vectorizer.get_feature_names()))#length of all tokens\n","np.array(vectorizer.get_feature_names()[0:200])#the first 200 tokens"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vv6UVT8FOfe4","colab_type":"text"},"source":["spliting the bag of words into a train valid and testset"]},{"cell_type":"code","metadata":{"id":"4VW4EENZOfe5","colab_type":"code","colab":{}},"source":["X_train = X[0:5000]\n","y_train = y[0:5000]\n","X_val = X[5000:10000]\n","y_val = y[5000:10000]\n","X_test = X[10000:15000]\n","y_test = y[10000:15000]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l8egWs4JOfe-","colab_type":"text"},"source":["the data more or less balanced"]},{"cell_type":"code","metadata":{"id":"hkjuPtSGOfe_","colab_type":"code","colab":{}},"source":["print(np.unique(y_train,return_counts=True))\n","print(np.unique(y_val,return_counts=True))\n","print(np.unique(y_test,return_counts=True))\n","print(X_train.shape)\n","print(X_val.shape)\n","print(X_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sLT32ZjeOffF","colab_type":"text"},"source":["bag of words for the first five reviews (very sparse representation), 5000 observations, with 6000 features\n"]},{"cell_type":"code","metadata":{"id":"t_nrYqisOffH","colab_type":"code","colab":{}},"source":["print(X_train[0:5])\n","print(X_train.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OySVMPm_OffM","colab_type":"text"},"source":["let's train a radomforest on the bag of words features of the train set"]},{"cell_type":"code","metadata":{"id":"zqBynwKWOffQ","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import RandomForestClassifier\n","classifier = RandomForestClassifier(n_estimators=300, random_state=36)  \n","classifier.fit(X_train, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"odquYZ9rOffY","colab_type":"text"},"source":["prediction on the test set with the accuracy and the confusion matrix"]},{"cell_type":"code","metadata":{"id":"bp5ZBTfpOffZ","colab_type":"code","colab":{}},"source":["y_pred = classifier.predict(X_test)  \n","print(pd.crosstab(y_test, y_pred,rownames=['true'], colnames=['pred']))\n","print(\"Acc = \",np.sum(y_test==y_pred)/len(y_test))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DSu20wPwOffe","colab_type":"text"},"source":["## Neural network with wordembedding"]},{"cell_type":"code","metadata":{"id":"3HMe0a0GOffg","colab_type":"code","colab":{}},"source":["X_train = documents[0:5000]\n","y_train = y[0:5000]\n","X_val = documents[5000:10000]\n","y_val = y[5000:10000]\n","X_test = documents[10000:15000]\n","y_test = y[10000:15000]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7sEMGLVWOffj","colab_type":"code","colab":{}},"source":["X_train[0]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dXv7H2cgOffm","colab_type":"text"},"source":["In the next cell we tokenize all unique words in the reviews and transform them into a sequence of the corresponding integer number of the token that belongs to the word. For example we cound transform \"the\" into the number 7. Then we take the length of the longest review and we zeropad all other reviews to that length of the longest review, so all reviews have the same length. With the max_words parameter you can control how many words you want to use, in this case we choose the 6000 most frequent words."]},{"cell_type":"code","metadata":{"id":"G35l_6qjOffo","colab_type":"code","colab":{}},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","max_words=6000\n","\n","# fit tokenizer on all reviews\n","total_reviews = documents\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(total_reviews) \n","\n","\n","# transform tokens to a sequence of integers\n","X_train_tokens =  tokenizer.texts_to_sequences(X_train)\n","X_val_tokens = tokenizer.texts_to_sequences(X_val)\n","X_test_tokens = tokenizer.texts_to_sequences(X_test)\n","\n","# max length of all reviews\n","max_length = max([len(s.split()) for s in total_reviews])\n","print(\"longest review:\",max_length,\"words\")\n","\n","# define vocabulary size (unique words in all reviews)\n","vocab_size = len(tokenizer.word_index) + 1\n","print(\"number of unique words/tokens:\",vocab_size)\n","\n","\n","# zeropad the sequences to have the \"same\" length \n","X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')\n","X_val_pad = pad_sequences(X_val_tokens, maxlen=max_length, padding='post')\n","X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ib7farhIOffv","colab_type":"text"},"source":["this is how our input for the neural network will look like, just a sequence of integer numbers"]},{"cell_type":"markdown","metadata":{"id":"qPiIV1SMOff2","colab_type":"text"},"source":["all unique tokens with the corresponding number"]},{"cell_type":"code","metadata":{"id":"3fskOK_MOff3","colab_type":"code","colab":{}},"source":["X_train_pad[0]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OvZLYheTIwhL","colab_type":"text"},"source":["Lets look at the first 200 most frquent words."]},{"cell_type":"code","metadata":{"id":"cVjMy-kNHrbi","colab_type":"code","colab":{}},"source":["np.array(list(tokenizer.word_index)[0:200])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qx-QPpLjOff7","colab_type":"text"},"source":["definition of the network with an embedding layer in the input that maps the numbers (words) into vectors of the same size.\n","architecture inspired by: https://www.tensorflow.org/tutorials/keras/basic_text_classification\n"]},{"cell_type":"code","metadata":{"id":"pCNxC8sZOff8","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding,GlobalAveragePooling1D,Dropout,GlobalMaxPool1D\n","\n","EMBEDDING_DIM = 30\n","\n","model = Sequential()\n","model.add(Embedding(max_words, EMBEDDING_DIM, input_length=(None)))\n","model.add(GlobalAveragePooling1D())\n","model.add(Dropout(0.5))\n","model.add(Dense(20, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z1WITi5IOfgA","colab_type":"code","colab":{}},"source":["history=model.fit(X_train_pad, y_train, batch_size=64, epochs=40, validation_data=(X_val_pad, y_val), verbose=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i9RaUyr5OfgE","colab_type":"code","colab":{}},"source":["# summarize history \n","plt.plot(history.history['acc'])\n","plt.plot(history.history['val_acc'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'valid'], loc='lower right')\n","plt.show()\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'valid'], loc='upper right')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QhrS0yPLOfgI","colab_type":"text"},"source":["prediction on the test set with the accuracy and the confusion matrix. Are we better than the RF baseline?"]},{"cell_type":"code","metadata":{"id":"ATsMpPPwOfgJ","colab_type":"code","colab":{}},"source":["y_pred = model.predict_classes(X_test_pad)  \n","y_pred=y_pred.reshape(len(y_pred))\n","print(pd.crosstab(y_test, y_pred,rownames=['true'], colnames=['pred']))\n","print(\"Acc = \",np.sum(y_test==y_pred)/len(y_test))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R_a9qkStOfgN","colab_type":"text"},"source":["\n","### We need to go deeper! \n","Write your own review or test the model on some reviews of Inception or some other movie, note that we can use an arbitrary length for the review.  \n","Visit: https://www.imdb.com/title/tt1375666/reviews?ref_=tt_ov_rt  \n","remeber 0 is negative and 1 is postive"]},{"cell_type":"code","metadata":{"id":"BfWmcDquOfgO","colab_type":"code","colab":{}},"source":["#paste new reviews here in the sample list\n","sample=list([\"Inception is truly one of a kind. A concept which has long gestated in Christopher Nolan's mind, his eye for drama mixed with his large scale sensibilities ring true in Blockbuster season making Inception a true original in the sea of reboots, remakes and sequels.To try and explain Inceptions many plot twists and incredibly intelligent arcs, would be a foolish task. As Nolan himself has been reluctant to. The best way to approach the film would be with an open mind, if you are prepared to be taken on a ride of a lifetime, then trust that you 100% will. If Avatar was a seminal film in technology (although coming out as a rather poor film, in my opinion), then Inception is seminal in it's storytelling. With a 148 minute running time, you would expect a lot to take place, but what you wouldn't expect is the pace of it all. I did not think at one time in the film about how long was left. I was simply blown away by the depth in every single part of the film. If my enthusiasm for the storytelling aspect of the film has left you worried about the spectacle, then don't worry. They are, as hinted in the trailer, incredible, looking real and unbelievable simultaneously. The most pleasing thing about the action set pieces, is that they are genuinely used to illustrate the story, rather than to blow stuff up a la Michael Bay. With this complex movie in it's high concept, a stellar cast is needed. And Nolan as always, delivers with just that. This is vintage DiCaprio, perhaps only equalled in The Aviator, which is even more impressive as his role as Cobb in Inception is not a showy one, needing DiCaprio to be the constant at the centre of the film. And he pulls off Cobb's emotional contradictions sublimely. The rest of the cast members all shine in parts of the films, Cillian Murphy shows off his usually non-existent tender side, Gordon-Levitt bottles his usual charm for his confidently reserved turn as the reliable Arthur, Watanabe is devilish as the seemingly ambiguous Saito, Page shows why she's the next big female star and Tom Hardy revels in being the comic relief of the film compared to his recent turns as decidedly psychopathic characters. Overall, Nolan has indeed surpassed himself. He has created a world as expansive as his Gotham, a plot dwarfing the intricacies of Memento and one which blows The Prestige's cinematic reveal out of the water. This is truly unmissable cinema. Revel in it, we've still got to wait a whole two years before Batman 3.\",\n","             \"Based on reviews I was hoping this was a different American film in the sense that it will have substance, subtlety and that it will make me think. It did not, it did not and it did not again. It is your typical Hollywood flick with car chases, shooting galore, explosions, fistfights, pretty boys and girls - the whole nine yards to sell tickets to the ADHD generation of teenagers. I gave it 3 stars - instead of just 2 - because the special effects are absolutely astonishing. This film wishes to be clever but really is not and does not make a lot of sense either. It reminded me of the teachers/professors who were confusing on purpose in order to make us believe they are smarter than we were.\"])\n","sample_tokens=tokenizer.texts_to_sequences(sample)\n","#print(np.array(sample_tokens))\n","print(model.predict(np.array(sample_tokens[0:1])))\n","print(model.predict(np.array(sample_tokens[1:2])))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oHRvJkQRxnXE","colab_type":"text"},"source":["### Now it's your turn\n","\n","\n","\n","*   Play around with the EMBEDDING_DIM, the max_words paramter and the network architecture.\n","*   How does the test performace change if you use globalmaxpooling after the embedding?\n","*   How does it affect the validation performace?\n","*   Think about if the sentences still make sense after the preprocessing."]},{"cell_type":"code","metadata":{"id":"g4qfbtubxd8_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"phMBGCA0OfgR","colab_type":"text"},"source":["### Optional: Neural network with wordembedding (inception like) with Keras functional api  "]},{"cell_type":"code","metadata":{"id":"UxlHDe18OfgS","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout, Embedding, Conv1D,GlobalMaxPooling1D,GlobalAveragePooling1D\n","EMBEDDING_DIM = 30\n","\n","\n","a = Input(shape=(max_length,))\n","x = Embedding(max_words, EMBEDDING_DIM)(a)\n","x1 = Conv1D(filters=50,kernel_size=(3),activation=\"relu\",padding=\"same\")(x)\n","x2 = Conv1D(filters=50,kernel_size=(5),activation=\"relu\",padding=\"same\")(x)\n","x3 = Conv1D(filters=50,kernel_size=(7),activation=\"relu\",padding=\"same\")(x)\n","\n","g1 = GlobalAveragePooling1D()(x1)\n","g2 = GlobalAveragePooling1D()(x2)\n","g3 = GlobalAveragePooling1D()(x3)\n","conc= Concatenate()([g1,g2,g3])\n","conc = Dropout(0.5)(conc)\n","out= Dense(1, activation='sigmoid')(conc)\n","model = Model(inputs=a, outputs=out)\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YaFMf-8EOfgW","colab_type":"code","colab":{}},"source":["history=model.fit(X_train_pad, y_train, batch_size=64, epochs=10, validation_data=(X_val_pad, y_val), verbose=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"63SJzB8AOfgZ","colab_type":"code","colab":{}},"source":["# summarize history \n","plt.plot(history.history['acc'])\n","plt.plot(history.history['val_acc'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'valid'], loc='lower right')\n","plt.show()\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'valid'], loc='upper right')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"batRfbrgOfgc","colab_type":"code","colab":{}},"source":["y_pred = model.predict(X_test_pad)  \n","y_pred=np.reshape(np.round(y_pred,0),(len(y_pred)))\n","print(pd.crosstab(y_test, y_pred,rownames=['true'], colnames=['pred']))\n","print(\"Acc = \",np.sum(y_test==y_pred)/len(y_test))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IkeseknUziDH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}