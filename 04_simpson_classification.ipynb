{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FNvzsBhVypza"
   },
   "source": [
    "## Simpsons characters classification \n",
    "\n",
    "In this notebook we try to classify images of different simpsons characters. The characters are 'abraham_grampa_simpson', 'apu_nahasapeemapetilon', 'bart_simpson', 'charles_montgomery_burns', 'chief_wiggum', 'homer_simpson', 'krusty_the_clown', 'lisa_simpson', 'marge_simpson', 'milhouse_van_houten', 'moe_szyslak', 'ned_flanders', 'principal_skinner' and 'sideshow_bob'.\n",
    "This dataset was preprocessed in an other notebook, it is splitted into a train val and testset and resized into 80x80 pixels and all characters have more than 600 images in total. The whole dataset with the original size can be found here https://www.kaggle.com/alexattia/the-simpsons-characters-dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YCCwPN1J7iqK"
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLeRfAvImGyK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SBNSW2X-mGyT"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D,GlobalMaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-1ONKZAyd8F"
   },
   "outputs": [],
   "source": [
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4zmmohbIyd8l"
   },
   "outputs": [],
   "source": [
    "Data=pd.read_csv(\"Data.csv\")\n",
    "\n",
    "X_train=np.load(\"X_train.npy\")\n",
    "Y_train=np.load(\"Y_train.npy\")\n",
    "\n",
    "X_val=np.load(\"X_val.npy\")\n",
    "Y_val=np.load(\"Y_val.npy\")\n",
    "\n",
    "X_test=np.load(\"X_test.npy\")\n",
    "Y_test=np.load(\"Y_test.npy\")\n",
    "\n",
    "labels=Data[\"label\"].unique()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5sUVgw_G7oN4"
   },
   "source": [
    "Let's use the trainset to plot a random image of each character. You can see that the characters are easy recognizable. And all images are the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cVMzs5EJyd8u"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for i in range(0,len(np.unique(np.argmax(Y_train,axis=1)))):\n",
    "    rmd=np.random.choice(np.where(np.argmax(Y_train,axis=1)==i)[0],1)\n",
    "    plt.subplot(4,4,i+1)\n",
    "    img=X_train[rmd]\n",
    "    plt.imshow(img[0,:,:,:])\n",
    "    plt.title(labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d8p2GxmZ8H3z"
   },
   "source": [
    "In this cell we plot the label distribution of all sets. You clearly see that the label distribution in all sets is very similar. The biggest class in the trainigset is obviously homer and the smallest class is apu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2GYK3DVyd8-"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.bar(np.unique(np.argmax(Y_train,axis=1),return_counts=True)[0],np.unique(np.argmax(Y_train,axis=1),return_counts=True)[1]\n",
    "       ,tick_label=labels )\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"train distribution\")\n",
    "plt.subplot(1,3,2)\n",
    "plt.bar(np.unique(np.argmax(Y_val,axis=1),return_counts=True)[0],np.unique(np.argmax(Y_val,axis=1),return_counts=True)[1]\n",
    "       ,tick_label=labels )\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"val distribution\")\n",
    "plt.subplot(1,3,3)\n",
    "plt.bar(np.unique(np.argmax(Y_test,axis=1),return_counts=True)[0],np.unique(np.argmax(Y_test,axis=1),return_counts=True)[1]\n",
    "       ,tick_label=labels )\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"test distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zeyypv_5yd9T"
   },
   "source": [
    "### Random guessing\n",
    "\n",
    "Let's build our first \"classifier\", what would be the accuracy if we would just random guess one of the labels for all testimages. Note that here, every character has the same chance to be predicted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w1N9YUfByd9e"
   },
   "outputs": [],
   "source": [
    "random_pred=np.zeros((len(X_test)),dtype=\"int64\")\n",
    "for i in range (0,len(X_test)):\n",
    "    random_pred[i]=np.random.choice(np.arange(0,max(np.argmax(Y_test,axis=1)+1)),1)\n",
    "acc=np.average(random_pred==np.argmax(Y_test,axis=1))\n",
    "res1 = pd.DataFrame(\n",
    "          {'Acc' : acc}, index=['random_guessing']\n",
    ")\n",
    "res1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KHNxyZhwyd9t"
   },
   "source": [
    "### Weighted random guessing\n",
    "\n",
    "Now let's build an other classifier, instead of just random guessing we now want to use the class distribution of the trainset, this means the chances that we predict homer are higher than that we predict apu and so on. Note that we assume that the testset is the same as the training here which is not always the case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sk0neC1wyd9w"
   },
   "outputs": [],
   "source": [
    "class_probs=np.unique(np.argmax(Y_train,axis=1),return_counts=True)[1]/len(Y_train)\n",
    "weighted_random_pred=np.zeros((len(X_test)),dtype=\"int64\")\n",
    "for i in range (0,len(X_test)):\n",
    "    weighted_random_pred[i]=np.random.choice(np.arange(0,max(np.argmax(Y_test,axis=1)+1)),1,p=class_probs)\n",
    "acc= np.average(weighted_random_pred==np.argmax(Y_test,axis=1))\n",
    "res2 = pd.DataFrame(\n",
    "          {'Acc' : acc}, index=['weighted_random_guessing']\n",
    ")\n",
    "pd.concat([res1,res2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9Wqwk_myd-N"
   },
   "source": [
    "### All max class\n",
    "\n",
    "The next \"classifier\", is just predicting every image to the biggest class, in our case this is homer. What is the accuracy if we just predict \"homer\" for all test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fy6jrCNJyd-Q"
   },
   "outputs": [],
   "source": [
    "idx=np.where(np.unique(np.argmax(Y_train,axis=1),return_counts=True)[1]==max(np.unique(np.argmax(Y_train,axis=1),return_counts=True)[1]))\n",
    "max_class=np.unique(np.argmax(Y_train,axis=1),return_counts=True)[0][idx]\n",
    "#print(max_class)\n",
    "acc=np.average(max_class==np.argmax(Y_test,axis=1))\n",
    "res3 = pd.DataFrame(\n",
    "          {'Acc' : acc}, index=['all_max_class']\n",
    ")\n",
    "pd.concat([res1,res2,res3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IUlqOPDyd-e"
   },
   "source": [
    "### RF with HOG features\n",
    "\n",
    "Let's use the fist real classifier. In the next cells we extract the histograms of oriented gradients of every 20x20 pxiel patch (the parameter orientations is the number of histograms you want to extract from each patch, and the pixel_per_cell parameter defines how big a patch is). Then we use a random forest model and train it on the hog featues of the the training data and use the trained model to predict the class of the images based on the hog features of the testdata. Finally we calculate the accuracy on the testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4qykPB0Myd-p"
   },
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "\n",
    "fd, hog_image = hog(X_train[0], orientations=5, pixels_per_cell=(20, 20),\n",
    "                    cells_per_block=(1, 1), visualize=True, multichannel=True)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "ax1.imshow(X_train[0], cmap=plt.cm.gray)\n",
    "ax1.set_title('Input image')\n",
    "\n",
    "# Rescale histogram for better display\n",
    "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "ax2.set_title('Histogram of Oriented Gradients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mNxG73lbyd-z"
   },
   "outputs": [],
   "source": [
    "hog_features_train=np.zeros((len(X_train),fd.shape[0]))\n",
    "for i in tqdm(range(0,len(X_train))):\n",
    "  fd, hog_image = hog(X_train[i], orientations=5, pixels_per_cell=(20, 20),\n",
    "                    cells_per_block=(1, 1), visualize=True, multichannel=True)\n",
    "  hog_features_train[i]=fd\n",
    "  \n",
    "hog_features_test=np.zeros((len(X_test),fd.shape[0]))\n",
    "for i in tqdm(range(0,len(X_test))):\n",
    "  fd, hog_image = hog(X_test[i], orientations=5, pixels_per_cell=(20, 20),\n",
    "                    cells_per_block=(1, 1), visualize=True, multichannel=True)\n",
    "  hog_features_test[i]=fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aR9E9-kmyd_J"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100,random_state=22)\n",
    "clf.fit(hog_features_train, np.argmax(Y_train,axis=1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQuuOccfyd_Z"
   },
   "outputs": [],
   "source": [
    "pred=clf.predict(hog_features_test)\n",
    "acc=np.average(pred==np.argmax(Y_test,axis=1))\n",
    "res4 = pd.DataFrame(\n",
    "          {'Acc' : acc}, index=['RF_with_hog']\n",
    ")\n",
    "pd.concat([res1,res2,res3,res4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6zT55jVVyd_o"
   },
   "source": [
    "### RF with colorhist featues\n",
    "\n",
    "In the next cells we extract the colorhistogram of each colorchannel. We choose a binsize of 12 and just extract 12 numbers for each channel, with 3 channels, we have 36 features per image. We again use a random forest model and train it on the colorhistogram-features of the the training data and use the trained model to predict the class of the images based on the colorhistogram-features of the testdata. Finally we calculate the accuracy on the testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SqLxlVkJyd_1"
   },
   "outputs": [],
   "source": [
    "bin_size=12\n",
    "\n",
    "col_hist_train=np.zeros((len(X_train),3*bin_size))\n",
    "for i in tqdm(range(0,len(X_train))):\n",
    "    col_hist_1=np.histogram(X_train[i,:,:,0],range=[0,255],bins=bin_size)[0]\n",
    "    col_hist_2=np.histogram(X_train[i,:,:,1],range=[0,255],bins=bin_size)[0]\n",
    "    col_hist_3=np.histogram(X_train[i,:,:,2],range=[0,255],bins=bin_size)[0]\n",
    "    col_hist_train[i]=np.concatenate([col_hist_1,col_hist_2,col_hist_3])\n",
    "\n",
    "col_hist_test=np.zeros((len(X_test),3*bin_size))\n",
    "for i in tqdm(range(0,len(X_test))):\n",
    "    col_hist_1=np.histogram(X_test[i,:,:,0],range=[0,255],bins=bin_size)[0]\n",
    "    col_hist_2=np.histogram(X_test[i,:,:,1],range=[0,255],bins=bin_size)[0]\n",
    "    col_hist_3=np.histogram(X_test[i,:,:,2],range=[0,255],bins=bin_size)[0]\n",
    "    col_hist_test[i]=np.concatenate([col_hist_1,col_hist_2,col_hist_3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NSd6HRGQyeAW"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100,random_state=22)\n",
    "clf.fit(col_hist_train, np.argmax(Y_train,axis=1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aINLz47AyeAg"
   },
   "outputs": [],
   "source": [
    "pred=clf.predict(col_hist_test)\n",
    "acc=np.average(pred==np.argmax(Y_test,axis=1))\n",
    "res5 = pd.DataFrame(\n",
    "          {'Acc' : acc}, index=['RF_with_colhist']\n",
    ")\n",
    "pd.concat([res1,res2,res3,res4,res5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FpcLJ2W8yeA1"
   },
   "source": [
    "### Fully connected neural network\n",
    "\n",
    "In the next cells we want to use a fully connected neural network. For this we first normalize the pixelvalues to be in the range from -1 to 1. Then we need to flatten the imput, note that we ignoire the 2d structure of the image here. We use a neural network with two hidden layers with the nodesizes of 800 and 200 and use the relu non-linearity. Finally we predict the probabilities for the 14 character with the softmax activation. As loss function we use the categorical crossentropy. We use a batchsize of 64 and fit for 5 epochs. We use the trainset to learn the weights and validate our performance on the validationset. For an estimation of the performace on new unseen data we predict the testset and check the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qv7BzMc3TA8y"
   },
   "outputs": [],
   "source": [
    "X_train=np.array(X_train,dtype=\"float32\")\n",
    "X_train=((X_train/255)-0.5)*2\n",
    "\n",
    "X_val=np.array(X_val,dtype=\"float32\")\n",
    "X_val=((X_val/255)-0.5)*2\n",
    "\n",
    "X_test=np.array(X_test,dtype=\"float32\")\n",
    "X_test=((X_test/255)-0.5)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BoKTw448mGz-"
   },
   "outputs": [],
   "source": [
    "\n",
    "model  =  Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(80,80,3)))\n",
    "model.add(Dense(800))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(14))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V1J60Sle73FA"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, \n",
    "                  batch_size=64, \n",
    "                  epochs=5,\n",
    "                  verbose=2,\n",
    "                  shuffle=True,\n",
    "                  validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lb5zvCsZ73B6"
   },
   "outputs": [],
   "source": [
    "acc=np.average(model.predict_classes(X_test)==np.argmax(Y_test,axis=1))\n",
    "res6 = pd.DataFrame({'Acc' : acc}, index=['FCN_on_pixel'])\n",
    "pd.concat([res1,res2,res3,res4,res5,res6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6fbbrV0_dq4"
   },
   "source": [
    "### VGG feature extraction and RF \n",
    "\n",
    "Now we will use a neural network (VGG16) that was trainend on Imagenet and we will only use the convolutional part to extract featues for our simpson images. With the features we will train and random forest classifier. Note that this network is trained to classify animals, vehicles and plants and was trained on a very large dataset. Let's see if we can extract useful features to decide which simpson character is on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovp13EAP_cLR"
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=False,input_shape=(80,80,3))\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPL_gRGB72-s"
   },
   "outputs": [],
   "source": [
    "#X_train_vgg_features=base_model.predict(X_train)\n",
    "#X_val_vgg_features=base_model.predict(X_val)\n",
    "#X_test_vgg_features=base_model.predict(X_test)\n",
    "\n",
    "#X_train_vgg_features=X_train_vgg_features.reshape((len(X_train),512*2*2))\n",
    "#X_val_vgg_features=X_val_vgg_features.reshape((len(X_val),512*2*2))\n",
    "#X_test_vgg_features=X_test_vgg_features.reshape((len(X_test),512*2*2))\n",
    "\n",
    "#takes a lot of time\n",
    "\n",
    "# load results\n",
    "X_train_vgg_features=np.load(\"X_train_vgg_features.npy\")\n",
    "X_val_vgg_features=np.load(\"X_val_vgg_features.npy\")\n",
    "X_test_vgg_features=np.load(\"X_test_vgg_features.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SmybI6CZ722f"
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,random_state=22)\n",
    "clf.fit(X_train_vgg_features, np.argmax(Y_train,axis=1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CnxqWKLj72y5"
   },
   "outputs": [],
   "source": [
    "pred=clf.predict(X_test_vgg_features)\n",
    "acc=np.average(pred==np.argmax(Y_test,axis=1))\n",
    "res7 = pd.DataFrame(\n",
    "          {'Acc' : acc}, index=['RF_on_vgg_features']\n",
    ")\n",
    "pd.concat([res1,res2,res3,res4,res5,res6,res7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pfFeb4unS0BE"
   },
   "source": [
    "### Tranfer learning after VGG extraction (only if you use colab)\n",
    "\n",
    "In this section we will again use a neural network (VGG16) that was trainend on Imagenet and this time we will add two fully connected layer on top of the features extraction part. We will freeze the weights of the convolutional part and only train the fully connected part that we added. We will predict the probabilities for the 14 character with the softmax activation. As loss function we use the categorical crossentropy. We use a batchsize of 64 and fit for 5 epochs. We use the trainset to learn the weights and validate our performance on the validationset. For an estimation of the performace on new unseen data we predict the testset and check the performance. Note that the training of the network may take a lot of time if you run it on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c41U1t_smG0A"
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=False,input_shape=(80,80,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NOvRz_1smG0O"
   },
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(400, activation='relu')(x)\n",
    "x = Dense(200, activation='relu')(x)\n",
    "\n",
    "\n",
    "predictions = Dense(max(Data[\"Klasse\"])+1, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9eneXgyZmG0Q",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mPZ9LahVmG0e"
   },
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "crPrg8ZKmG0j"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQFA6aMpmG0n"
   },
   "outputs": [],
   "source": [
    "for i, layer in enumerate(model.layers):\n",
    "   print(i, layer.name,layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r0cP8bJVmG0u",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaK0MD4dTNBf"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, \n",
    "                  batch_size=64, \n",
    "                  epochs=5,\n",
    "                  verbose=2,\n",
    "                  shuffle=True,\n",
    "                  validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FB3sAVPvTsiQ"
   },
   "outputs": [],
   "source": [
    "acc=np.average(np.argmax(model.predict(X_test),axis=1)==np.argmax(Y_test,axis=1))\n",
    "res8 = pd.DataFrame(\n",
    "          {'Acc' : acc}, index=['transfer_learning_on_vgg_features']\n",
    ")\n",
    "pd.concat([res1,res2,res3,res4,res5,res6,res7,res8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LOjZirFJW9gJ"
   },
   "source": [
    "### Now it's your turn\n",
    "\n",
    "\n",
    "\n",
    "*   Take the best model and check the indvudial class performace for each class.\n",
    "*   Look at some wrong predictions.\n",
    "*   Try to improve the performace on the testset with a different model.  \n",
    "*   *Hints:  You may want to use a deeper neutal network, or combine the features for the random forest. Maybe data augmntation could improve the performace or a CNN form scratch may work well.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vM1rgCeMV7-4"
   },
   "outputs": [],
   "source": [
    "### acc per class\n",
    "pred=np.argmax(model.predict(X_test),axis=1)\n",
    "for i in range(0,len(labels)):\n",
    "  print(labels[i],np.average(pred[np.where(np.argmax(Y_test,axis=1)==i)]==i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJApAI-RWhWS"
   },
   "outputs": [],
   "source": [
    "X_test_unnorm=np.load(\"X_test.npy\")\n",
    "idx=np.where(pred!=np.argmax(Y_test,axis=1))[0]\n",
    "rmd=np.random.choice(idx,1)\n",
    "print(\"predicted:\",labels[pred[rmd]])\n",
    "print(\"true:\",labels[np.argmax(Y_test,axis=1)[rmd]])\n",
    "plt.imshow(np.squeeze(X_test_unnorm[rmd]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_A97no8X-H8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "04_simpson_classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
